<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>《动手学深度学习》笔记 Task02_文本预处理；语言模型；循环神经网络基础</title>
    <url>/2020/02/13/article2/</url>
    <content><![CDATA[<h1 id="一、文本预处理"><a href="#一、文本预处理" class="headerlink" title="一、文本预处理"></a>一、文本预处理</h1><p><strong>主要内容：<br>1.文本预处理的解释<br>2.文本预处理的过程<br>3.现有分词工具</strong></p>
<h2 id="1-文本预处理的解释"><a href="#1-文本预处理的解释" class="headerlink" title="1.文本预处理的解释"></a>1.文本预处理的解释</h2><p>文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：<br><strong>1.读入文本<br>2.分词<br>3.建立字典，将每个词映射到一个唯一的索引（index）<br>4.将文本从词的序列转换为索引的序列，方便输入模型</strong></p>
<h2 id="2-文本预处理的过程"><a href="#2-文本预处理的过程" class="headerlink" title="2.文本预处理的过程"></a>2.文本预处理的过程</h2><h3 id="2-1读入文本"><a href="#2-1读入文本" class="headerlink" title="2.1读入文本"></a>2.1读入文本</h3><p>我们用一部英文小说，即H. G. Well的Time Machine，作为示例，展示文本预处理的具体过程。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'/home/kesci/input/timemachine7163/timemachine.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = [re.sub(<span class="string">'[^a-z]+'</span>, <span class="string">' '</span>, line.strip().lower()) <span class="keyword">for</span> line <span class="keyword">in</span> f]</span><br><span class="line">    <span class="keyword">return</span> lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line">print(<span class="string">'# sentences %d'</span> % len(lines))</span><br></pre></td></tr></table></figure>
<p>out：<br>sentences 3221</p>
<h3 id="2-2分词"><a href="#2-2分词" class="headerlink" title="2.2分词"></a>2.2分词</h3><p>我们对每个句子进行分词，也就是将一个句子划分成若干个词（token），转换为一个词的序列。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentences, token=<span class="string">'word'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Split sentences into word or char tokens"""</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">'word'</span>:</span><br><span class="line">        <span class="keyword">return</span> [sentence.split(<span class="string">' '</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">'char'</span>:</span><br><span class="line">        <span class="keyword">return</span> [list(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'ERROR: unkown token type '</span>+token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">tokens[<span class="number">0</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>out：<br>[[‘the’, ‘time’, ‘machine’, ‘by’, ‘h’, ‘g’, ‘wells’, ‘’], [‘’]]</p>
<h3 id="2-3建立字典"><a href="#2-3建立字典" class="headerlink" title="2.3建立字典"></a>2.3建立字典</h3><p>为了方便模型处理，我们需要将字符串转换为数字。因此我们需要先构建一个字典（vocabulary），将每个词映射到一个唯一的索引编号。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">        counter = count_corpus(tokens)  <span class="comment"># : </span></span><br><span class="line">        self.token_freqs = list(counter.items())</span><br><span class="line">        self.idx_to_token = []</span><br><span class="line">        <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">            <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">            self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.unk = <span class="number">0</span></span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>]</span><br><span class="line">        self.idx_to_token += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> self.token_freqs</span><br><span class="line">                        <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.idx_to_token]</span><br><span class="line">        self.token_to_idx = dict()</span><br><span class="line">        <span class="keyword">for</span> idx, token <span class="keyword">in</span> enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] = idx</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    tokens = [tk <span class="keyword">for</span> st <span class="keyword">in</span> sentences <span class="keyword">for</span> tk <span class="keyword">in</span> st]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)  <span class="comment"># 返回一个字典，记录每个词的出现次数</span></span><br></pre></td></tr></table></figure>
<p>我们看一个例子，这里我们尝试用Time Machine作为语料构建字典</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vocab = Vocab(tokens)</span><br><span class="line">print(list(vocab.token_to_idx.items())[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>out:<br>[(‘’, 0), (‘the’, 1), (‘time’, 2), (‘machine’, 3), (‘by’, 4), (‘h’, 5), (‘g’, 6), (‘wells’, 7), (‘i’, 8), (‘traveller’, 9)]</p>
<h3 id="2-3将词转为索引"><a href="#2-3将词转为索引" class="headerlink" title="2.3将词转为索引"></a>2.3将词转为索引</h3><p>使用字典，我们可以将原文本中的句子从单词序列转换为索引序列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>, <span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'words:'</span>, tokens[i])</span><br><span class="line">    print(<span class="string">'indices:'</span>, vocab[tokens[i]])</span><br></pre></td></tr></table></figure>
<p>out:<br>words: [‘the’, ‘time’, ‘traveller’, ‘for’, ‘so’, ‘it’, ‘will’, ‘be’, ‘convenient’, ‘to’, ‘speak’, ‘of’, ‘him’, ‘’]<br>indices: [1, 2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0]<br>words: [‘was’, ‘expounding’, ‘a’, ‘recondite’, ‘matter’, ‘to’, ‘us’, ‘his’, ‘grey’, ‘eyes’, ‘shone’, ‘and’]<br>indices: [20, 21, 22, 23, 24, 16, 25, 26, 27, 28, 29, 30]</p>
<h2 id="3-现有分词工具"><a href="#3-现有分词工具" class="headerlink" title="3.现有分词工具"></a>3.现有分词工具</h2><p>我们前面介绍的分词方式非常简单，它至少有以下几个缺点:<br>      1.标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了<br>      2.类似“shouldn’t”, “doesn’t”这样的词会被错误地处理<br>      3.类似”Mr.”, “Dr.”这样的词会被错误地处理<br>我们可以通过引入更复杂的规则来解决这些问题，但是事实上，有一些现有的工具可以很好地进行分词，我们在这里简单介绍其中的两个：spaCy和NLTK。</p>
<p>代码讲解和帮助文档链接：<a href="https://www.kesci.com/org/boyuai/workspace/project" target="_blank" rel="noopener">添加链接描述</a></p>
<p><strong>习题练习：</strong><br><img src="https://img-blog.csdnimg.cn/20200214150040815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="二、语言模型"><a href="#二、语言模型" class="headerlink" title="二、语言模型"></a>二、语言模型</h1><p><strong>主要内容：<br>1.语言模型的解释<br>2.n元语法<br>3.语言模型数据集的两种采样方法</strong></p>
<h2 id="1-语言模型的解释"><a href="#1-语言模型的解释" class="headerlink" title="1.语言模型的解释"></a>1.语言模型的解释</h2><p><img src="https://img-blog.csdnimg.cn/20200214150526777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="2-n元语法"><a href="#2-n元语法" class="headerlink" title="2.n元语法"></a>2.n元语法</h2><p><img src="https://img-blog.csdnimg.cn/2020021415060246.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>帮助理解n元语法的博客：<a href="https://blog.csdn.net/wangyangzhizhou/article/details/78651397" target="_blank" rel="noopener">添加链接描述</a></p>
<h2 id="3-时序数据的采样"><a href="#3-时序数据的采样" class="headerlink" title="3.时序数据的采样"></a>3.时序数据的采样</h2><p>在训练中我们需要每次随机读取小批量样本和标签。与之前章节的实验数据不同的是，时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”，即X=“想要有直升”，Y=“要有直升机”。</p>
<p>现在我们考虑序列“想要有直升机，想要和你飞到宇宙去”，如果时间步数为5，有以下可能的样本和标签：<br><img src="https://img-blog.csdnimg.cn/20200214150656835.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看到，如果序列的长度为T，时间步数为n，那么一共有 T-n 个合法的样本，但是这些样本有大量的重合，我们通常采用更加高效的采样方式。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。</p>
<h3 id="3-1随机采样"><a href="#3-1随机采样" class="headerlink" title="3.1随机采样"></a>3.1随机采样</h3><p>在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。</p>
<h3 id="3-2相邻采样"><a href="#3-2相邻采样" class="headerlink" title="3.2相邻采样"></a>3.2相邻采样</h3><p>在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻。</p>
<p>代码和帮助理解的文档链接：<a href="https://www.kesci.com/org/boyuai/workspace/project" target="_blank" rel="noopener">添加链接描述</a></p>
<p>习题练习：<br><img src="https://img-blog.csdnimg.cn/20200214151329997.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="三、循环神经网络基础"><a href="#三、循环神经网络基础" class="headerlink" title="三、循环神经网络基础"></a>三、循环神经网络基础</h1><p><strong>主要内容：<br>1.循环神经网络的介绍<br>2.循环神经网络的实现</strong></p>
<h2 id="1-循环神经网络的介绍"><a href="#1-循环神经网络的介绍" class="headerlink" title="1.循环神经网络的介绍"></a>1.循环神经网络的介绍</h2><p><img src="https://img-blog.csdnimg.cn/20200214152510369.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="1-1循环神经网络的构造"><a href="#1-1循环神经网络的构造" class="headerlink" title="1.1循环神经网络的构造"></a>1.1循环神经网络的构造</h3><p><img src="https://img-blog.csdnimg.cn/2020021415254236.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="1-2裁剪梯度"><a href="#1-2裁剪梯度" class="headerlink" title="1.2裁剪梯度"></a>1.2裁剪梯度</h3><p><img src="https://img-blog.csdnimg.cn/20200214152626661.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="1-3困惑度"><a href="#1-3困惑度" class="headerlink" title="1.3困惑度"></a>1.3困惑度</h3><p>我们通常使用困惑度（perplexity）来评价语言模型的好坏。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，<br><strong>1.最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；<br>2.最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；<br>3.基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</strong><br>显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典大小vocab_size。</p>
<h2 id="2-循环神经网络的实现"><a href="#2-循环神经网络的实现" class="headerlink" title="2.循环神经网络的实现"></a>2.循环神经网络的实现</h2><p>代码和解释帮助文档：<a href="https://www.kesci.com/org/boyuai/project/5e42c3ad5f2816002ce979b0" target="_blank" rel="noopener">添加链接描述</a></p>
<p><strong>习题练习：</strong><br><img src="https://img-blog.csdnimg.cn/20200214153132171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>参考资料</strong></p>
<p>《动手学深度学习》中文版官网教材：<a href="http://zh.gluon.ai/" target="_blank" rel="noopener">添加链接描述</a><br>PyTorch中文文档：<a href="https://pytorch-cn.readthedocs.io/zh/stable/" target="_blank" rel="noopener">添加链接描述</a></p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>文本预处理</tag>
        <tag>语言模型</tag>
        <tag>循环神经网络基础</tag>
      </tags>
  </entry>
  <entry>
    <title>《动手学深度学习》笔记 Task01_线性回归；Softmax与分类模型、多层感知机</title>
    <url>/2020/02/13/article1/</url>
    <content><![CDATA[<h1 id="一、线性回归"><a href="#一、线性回归" class="headerlink" title="一、线性回归"></a><strong>一、线性回归</strong></h1><p><strong>主要内容：<br>1.线性回归的解释<br>2.线性回归模型的基本要素<br>3.线性回归模型的两种实现方式</strong></p>
<h2 id="1-线性回归的解释"><a href="#1-线性回归的解释" class="headerlink" title="1.线性回归的解释"></a>1.线性回归的解释</h2><p>线性回归，就是能够用一个直线较为精确地描述数据之间的关系。这样当出现新的数据的时候，就能够预测出一个简单的值。线性回归中最常见的就是房价的问题。一直存在很多房屋面积和房价的数据，如下图所示：<br><img src="https://img-blog.csdnimg.cn/20200214132232134.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>在这种情况下，就可以利用线性回归构造出一条直线来近似地描述放假与房屋面积之间的关系，从而就可以根据房屋面积推测出房价。<br><em>线性回归输出是一个连续值，因此适用于回归问题</em>。回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。与回归问题不同，分类问题中模型的最终输出是一个离散值。我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。softmax回归则适用于分类问题。<br>由于==线性回归和softmax回归都是单层神经网络==，它们涉及的概念和技术同样适用于大多数的深度学习模型。</p>
<h2 id="2-线性回归模型的基本要素"><a href="#2-线性回归模型的基本要素" class="headerlink" title="2.线性回归模型的基本要素"></a>2.线性回归模型的基本要素</h2><h2 id="2-1-模型"><a href="#2-1-模型" class="headerlink" title="2.1 模型"></a>2.1 模型</h2><p>为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。线性回归假设输出与各个输入之间是线性关系:<br><img src="https://img-blog.csdnimg.cn/20200214133203357.jpg" alt="在这里插入图片描述"></p>
<h2 id="2-2-模型训练"><a href="#2-2-模型训练" class="headerlink" title="2.2 模型训练"></a>2.2 模型训练</h2><p>接下来我们需要通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程叫作模型训练（model training）。下面我们介绍模型训练所涉及的3个要素。</p>
<h3 id="2-2-1-数据集"><a href="#2-2-1-数据集" class="headerlink" title="2.2.1 数据集"></a>2.2.1 数据集</h3><p>我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。</p>
<h3 id="2-2-2-损失函数"><a href="#2-2-2-损失函数" class="headerlink" title="2.2.2 损失函数"></a>2.2.2 损失函数</h3><p>模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为  i  的样本误差的表达式为<br><img src="https://img-blog.csdnimg.cn/2020021413334554.jpg" alt="在这里插入图片描述"></p>
<h3 id="2-2-3-优化函数-随机梯度下降"><a href="#2-2-3-优化函数-随机梯度下降" class="headerlink" title="2.2.3 优化函数 - 随机梯度下降"></a>2.2.3 优化函数 - 随机梯度下降</h3><p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。</p>
<p>在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch） B ，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。<img src="https://img-blog.csdnimg.cn/20200214133605266.jpg" alt="在这里插入图片描述"><br>学习率:  η 代表在每次优化中，能够学习的步长的大小<br>批量大小:  B 是小批量计算中的批量大小batch size</p>
<p>总结一下，优化函数的有以下两个步骤：<br>(i)初始化模型参数，一般来说使用随机初始化；<br>(ii)我们在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。</p>
<h2 id="2-3-模型预测"><a href="#2-3-模型预测" class="headerlink" title="2.3 模型预测"></a>2.3 模型预测</h2><p>模型训练完成后，我们将模型参数 在优化算法停止时的值分别记录 。注意，这里我们得到的并不一定是最小化损失函数的最优解  ，而是对最优解的一个近似。然后，我们就可以使用学出的线性回归模型来估算训练数据集以外任意一栋面积（平方米）为 area 、房龄（年）为 age 的房屋的价格了。这里的估算也叫作模型预测、模型推断或模型测试。</p>
<h2 id="3-线性回归模型的两种实现方式"><a href="#3-线性回归模型的两种实现方式" class="headerlink" title="3.线性回归模型的两种实现方式"></a>3.线性回归模型的两种实现方式</h2><p>方式一：从零开始的实现（推荐用来学习），能够更好的理解模型和神经网络底层的原理<br>方式二：使用pytorch的简洁实现，能够更加快速地完成模型的设计与实现<br>代码与讲解链接：<a href="https://www.kesci.com/org/boyuai/workspace/project" target="_blank" rel="noopener">添加链接描述</a></p>
<p><strong>习题练习：</strong><br><img src="https://img-blog.csdnimg.cn/20200214135309137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="二、Softmax与分类模型"><a href="#二、Softmax与分类模型" class="headerlink" title="二、Softmax与分类模型"></a><strong>二、Softmax与分类模型</strong></h1><p>线性回归模型适用于输出为连续值的情景。在另一类情景中，模型输出可以是一个像图像类别这样的离散值。对于这样的离散值预测问题，我们可以使用诸如softmax回归在内的分类模型。和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。本节以softmax回归模型为例，介绍神经网络中的分类模型。</p>
<p><strong>主要内容：<br>1.softmax回归的基本概念<br>2.softmax回归模型的两种实现，实现一个对Fashion-MNIST训练集中的图像数据进行分类的模型</strong></p>
<h2 id="1-softmax回归的基本概念"><a href="#1-softmax回归的基本概念" class="headerlink" title="1.softmax回归的基本概念"></a>1.softmax回归的基本概念</h2><h3 id="1-1分类问题"><a href="#1-1分类问题" class="headerlink" title="1.1分类问题"></a>1.1分类问题</h3><p>一个简单的图像分类问题，输入图像的高和宽均为2像素，色彩为灰度。<br>图像中的4像素分别记为x1, x2, x3, x4。<br>假设真实标签为狗、猫或者鸡，这些标签对应的离散值为y1, y2, y3。<br>我们通常使用离散的数值来表示类别，例如y1=1, y2=2, y3=3。</p>
<h3 id="1-2权重矢量"><a href="#1-2权重矢量" class="headerlink" title="1.2权重矢量"></a>1.2权重矢量</h3><p><img src="https://img-blog.csdnimg.cn/20200214141026943.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="1-3神经网络图"><a href="#1-3神经网络图" class="headerlink" title="1.3神经网络图"></a>1.3神经网络图</h3><p>下图用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出o1, o2, o3的计算都要依赖于所有的输入x1, x2, x3, x4，softmax回归的输出层也是一个全连接层。<br><img src="https://img-blog.csdnimg.cn/20200214141157927.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200214141421164.jpg" alt="在这里插入图片描述"></p>
<h3 id="1-4输出问题"><a href="#1-4输出问题" class="headerlink" title="1.4输出问题"></a>1.4输出问题</h3><p>直接使用输出层的输出有两个问题：<br>1.一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果o1=o3=10^3，那么输出值10却又表示图像类别为猫的概率很低。<br>2.另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。<br>softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：<img src="https://img-blog.csdnimg.cn/20200214141611912.jpg" alt="在这里插入图片描述"><br>其中<br><img src="https://img-blog.csdnimg.cn/20200214141706632.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>因此softmax运算不改变预测类别输出。</p>
<h3 id="1-4计算效率"><a href="#1-4计算效率" class="headerlink" title="1.4计算效率"></a>1.4计算效率</h3><h4 id="1-4-1单样本矢量计算表达式"><a href="#1-4-1单样本矢量计算表达式" class="headerlink" title="1.4.1单样本矢量计算表达式"></a>1.4.1单样本矢量计算表达式</h4><p>为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为<br><img src="https://img-blog.csdnimg.cn/20200214141831743.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>设高和宽分别为2个像素的图像样本i的特征为<br><img src="https://img-blog.csdnimg.cn/20200214141906591.jpg" alt="在这里插入图片描述"><br>输出层的输出为<br><img src="https://img-blog.csdnimg.cn/20200214141942367.jpg" alt="在这里插入图片描述"><br>预测为狗、猫或鸡的概率分布为<br><img src="https://img-blog.csdnimg.cn/20200214142013490.jpg" alt="在这里插入图片描述"><br>softmax回归对样本i分类的矢量计算表达式为<br><img src="https://img-blog.csdnimg.cn/2020021414210973.png" alt="在这里插入图片描述"></p>
<h4 id="1-4-2小批量矢量计算表达式"><a href="#1-4-2小批量矢量计算表达式" class="headerlink" title="1.4.2小批量矢量计算表达式"></a>1.4.2小批量矢量计算表达式</h4><p><img src="https://img-blog.csdnimg.cn/20200214142146320.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="1-5交叉熵损失函数"><a href="#1-5交叉熵损失函数" class="headerlink" title="1.5交叉熵损失函数"></a>1.5交叉熵损失函数</h3><p><img src="https://img-blog.csdnimg.cn/20200214142303851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>改善上述问题的一个方法是使用更适合衡量两个概率分布差异的测量函数。其中，交叉熵（cross entropy）是一个常用的衡量方法：<br><img src="https://img-blog.csdnimg.cn/20200214142323890.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200214142336511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="1-6模型训练和预测"><a href="#1-6模型训练和预测" class="headerlink" title="1.6模型训练和预测"></a>1.6模型训练和预测</h3><p>在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常，我们把预测概率最大的类别作为输出类别。如果它与真实类别（标签）一致，说明这次预测是正确的。我们将使用准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之比。</p>
<h2 id="2-softmax回归模型的两种实现"><a href="#2-softmax回归模型的两种实现" class="headerlink" title="2.softmax回归模型的两种实现"></a>2.softmax回归模型的两种实现</h2><p>方式一：从零开始实现，实现一个对Fashion-MNIST训练集中的图像数据进行分类的模型<br>方式二：使用pytorch重新实现softmax回归模型<br>代码与讲解链接：<a href="https://www.kesci.com/org/boyuai/workspace/project" target="_blank" rel="noopener">添加链接描述</a><br>帮助理解softmax函数的博客：<a href="https://blog.csdn.net/lz_peter/article/details/84574716" target="_blank" rel="noopener">添加链接描述</a></p>
<p><strong>习题练习：</strong><br><img src="https://img-blog.csdnimg.cn/20200214142934761.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="三、多层感知机"><a href="#三、多层感知机" class="headerlink" title="三、多层感知机"></a><strong>三、多层感知机</strong></h1><p>我们已经介绍了包括线性回归和softmax回归在内的单层神经网络。然而深度学习主要关注多层模型。我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。<br><strong>主要内容：<br>1.多层感知机的基本知识<br>2.使用多层感知机图像分类的两种实现</strong></p>
<h2 id="1-多层感知机的基本知识"><a href="#1-多层感知机的基本知识" class="headerlink" title="1.多层感知机的基本知识"></a>1.多层感知机的基本知识</h2><h3 id="1-1隐藏层"><a href="#1-1隐藏层" class="headerlink" title="1.1隐藏层"></a>1.1隐藏层</h3><p>多层感知机在单层神经网络的基础上引入了一到多个隐藏层（hidden layer）。<br>下图展示了一个多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。<br><img src="https://img-blog.csdnimg.cn/20200214143441170.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020021414362463.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="1-2激活函数"><a href="#1-2激活函数" class="headerlink" title="1.2激活函数"></a>1.2激活函数</h3><p>上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。<br>下面我们介绍几个常用的激活函数：</p>
<p><strong>ReLU函数</strong><br>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素 x ，该函数定义为<br><img src="https://img-blog.csdnimg.cn/20200214143743500.png" alt="在这里插入图片描述"><br>可以看出，ReLU函数只保留正数元素，并将负数元素清零。</p>
<p><strong>Sigmoid函数</strong><br>sigmoid函数可以将元素的值变换到0和1之间：<br><img src="https://img-blog.csdnimg.cn/2020021414383936.png" alt="在这里插入图片描述"><br><strong>tanh函数</strong><br>tanh（双曲正切）函数可以将元素的值变换到-1和1之间：<br><img src="https://img-blog.csdnimg.cn/20200214143917866.png" alt="在这里插入图片描述"><br>当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。</p>
<p><strong>关于激活函数的选择</strong><br>1.ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。</p>
<p>2.用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。</p>
<p>3.在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。</p>
<p>在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。</p>
<h3 id="1-3多层感知机"><a href="#1-3多层感知机" class="headerlink" title="1.3多层感知机"></a>1.3多层感知机</h3><p>多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。以单隐藏层为例并沿用本节之前定义的符号，多层感知机按以下方式计算输出：<br><img src="https://img-blog.csdnimg.cn/20200214144113836.png" alt="在这里插入图片描述"></p>
<h2 id="2-使用多层感知机图像分类的两种实现"><a href="#2-使用多层感知机图像分类的两种实现" class="headerlink" title="2.使用多层感知机图像分类的两种实现"></a>2.使用多层感知机图像分类的两种实现</h2><p>方式一：从零开始实现的实现<br>方式二：使用pytorch的简洁实现<br>代码与讲解链接：<a href="https://www.kesci.com/org/boyuai/workspace/project" target="_blank" rel="noopener">添加链接描述</a><br>帮助理解的博客：<a href="https://blog.csdn.net/fg13821267836/article/details/93405572" target="_blank" rel="noopener">添加链接描述</a></p>
<p><strong>习题练习：</strong><br><img src="https://img-blog.csdnimg.cn/20200214144346817.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVyX0NDWQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>参考资料：</strong><br>《动手学深度学习》中文版官网教材：<a href="http://zh.gluon.ai/" target="_blank" rel="noopener">添加链接描述</a><br>PyTorch中文文档：<a href="https://pytorch-cn.readthedocs.io/zh/stable/" target="_blank" rel="noopener">添加链接描述</a></p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>线性回归</tag>
        <tag>Softmax</tag>
        <tag>多层感知机</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/02/10/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
